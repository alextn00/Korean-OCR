{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "from keras import models, layers\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "path = '../json/'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "Korean_id = []\n",
    "Korean_text = []\n",
    "\n",
    "# Korean_2350 = []\n",
    "# with open(\"./KS1001/Korean_2350.txt\", \"r\", encoding='utf-8') as f :\n",
    "#     Korean_2350_origin = f.read()\n",
    "# for i in range(len(Korean_2350_origin)) :\n",
    "#     Korean_2350.append(Korean_2350_origin[i])\n",
    "# Korean_2350 = Korean_2350[0:2349]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in file_list : \n",
    "    filename = str(f)\n",
    "    filename_list = filename.split('.')\n",
    "    filename = filename_list[0]\n",
    "    with open('../json/' + filename + '.json','r',encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "    # print(file['bbox'][0]['x'])\n",
    "    # print(file['bbox'][0]['y'])\n",
    "    bbox = file['bbox']\n",
    "\n",
    "    for index, box in enumerate(bbox):\n",
    "        text = file['bbox'][index]['data']\n",
    "        id = file['bbox'][index]['id']\n",
    "        Korean_id.append(filename + \"_\" + str(id))\n",
    "        Korean_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Korean_train_id = Korean_id[0:600]\n",
    "Korean_train_text = Korean_text[0:600]\n",
    "\n",
    "Korean_val_id = Korean_id[600:900]\n",
    "Korean_val_text = Korean_text[600:900]\n",
    "\n",
    "Korean_test_id = Korean_id[900:]\n",
    "Korean_test_text = Korean_text[900:]\n",
    "\n",
    "syllable = list(set(Korean_text))\n",
    "\n",
    "del(Korean_id)\n",
    "del(Korean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.zeros((600, 32, 32, 3), 'uint8')\n",
    "x_val = np.zeros((300, 32, 32, 3), 'uint8')\n",
    "x_test = np.zeros((100, 32, 32, 3), 'uint8')\n",
    "\n",
    "for i, ID in enumerate(Korean_train_id) :\n",
    "    Image_addr = \"../process_data/\" + str(ID) + \".png\"\n",
    "    Korean_Image = Image.open(Image_addr)\n",
    "    Korean_Image = Korean_Image.resize((32, 32))\n",
    "    Korean_Image_Array = np.array(Korean_Image, 'uint8')\n",
    "    x_train[i] = Korean_Image_Array\n",
    "\n",
    "for i, ID in enumerate(Korean_val_id) :\n",
    "    Image_addr = \"../process_data/\" + str(ID) + \".png\"\n",
    "    Korean_Image = Image.open(Image_addr)\n",
    "    Korean_Image = Korean_Image.resize((32, 32))\n",
    "    Korean_Image_Array = np.array(Korean_Image, 'uint8')\n",
    "    x_val[i] = Korean_Image_Array\n",
    "\n",
    "for i, ID in enumerate(Korean_test_id) :\n",
    "    Image_addr = \"../process_data/\" + str(ID) + \".png\"\n",
    "    Korean_Image = Image.open(Image_addr)\n",
    "    Korean_Image = Korean_Image.resize((32, 32))\n",
    "    Korean_Image_Array = np.array(Korean_Image, 'uint8')\n",
    "    x_test[i] = Korean_Image_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')/255.0\n",
    "x_val = x_val.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')/255.0\n",
    "\n",
    "del(Korean_train_id)\n",
    "del(Korean_val_id)\n",
    "del(Korean_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_to_index = {syllable : index for index, syllable in enumerate(syllable)}\n",
    "index_to_syllable = {index : syllable for index, syllable in enumerate(syllable)}\n",
    "\n",
    "label_train = []\n",
    "label_val = []\n",
    "label_test = []\n",
    "\n",
    "for syllables in Korean_train_text : \n",
    "    if syllable_to_index.get(syllables) is not None: \n",
    "        label_train.extend([syllable_to_index[syllables]])\n",
    "\n",
    "for syllables in Korean_val_text : \n",
    "    if syllable_to_index.get(syllables) is not None : \n",
    "        label_val.extend([syllable_to_index[syllables]])\n",
    "\n",
    "for syllables in Korean_test_text : \n",
    "    if syllable_to_index.get(syllables) is not None : \n",
    "        label_test.extend([syllable_to_index[syllables]])\n",
    "\n",
    "del(Korean_train_text)\n",
    "del(Korean_val_text)\n",
    "del(Korean_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status initialize\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "\n",
    "# out of memory\n",
    "with tf.Graph().as_default():\n",
    "  gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_train = np.array(label_train, 'uint8')\n",
    "# label_val = np.array(label_val, 'uint8')\n",
    "# label_test = np.array(label_test, 'uint8')\n",
    "\n",
    "label_train = to_categorical(label_train)\n",
    "label_val = to_categorical(label_val)\n",
    "label_test = to_categorical(label_test)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(512, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(2349, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    model.fit(x_train, label_train, epochs = 20, batch_size = 128, validation_data=(x_val, label_val))\n",
    "#model.fit(x_train, label_train, epochs = 200, batch_size = 128, validation_data=x_val)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, label_test)\n",
    "print('test_loss:      ',test_loss)\n",
    "print('test_accuracy:  ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Korean_CNN_model.h5')\n",
    "\n",
    "with open(\"index_to_syllable.json\", \"w\") as json_file:\n",
    "   json.dump(index_to_syllable, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66d10cac04648a55565af852e0037a18062146ef3971972e375ca0f61f99b17e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
